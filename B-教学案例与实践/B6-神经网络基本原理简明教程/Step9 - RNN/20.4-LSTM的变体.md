<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.7 LSTM的变体

### 19.7.1 GRU 的基本概念

LSTM 存在很多变体，其中门控循环单元（Gated Recurrent Unit, GRU）是最常见的一种，也是目前比较流行的一种。GRU是由 [Cho](https://arxiv.org/pdf/1406.1078v3.pdf) 等人在2014年提出的，它对LSTM做了一些简化：

1. GRU 将 LSTM 原来的三个门简化成为两个：重置门 $r_t$（Reset Gate）和更新门 $z_t$ (Update Gate)。
2. GRU 不保留单元状态 $c_t$，只保留隐藏状态 $h_t$ 作为单元输出。
3. 重置门直接作用于前一时刻的隐藏状态 $h_{t-1}$。


### 19.7.2 GRU的前向计算

下图展示了GRU的单元结构：

<img src="../Images/19/gru_structure.png" />

它的前向计算过程如下：

1. 更新门
   $$
   z_t = \sigma(W_z\cdot[h_{t-1}, x_t])
   $$
2. 重置门
   $$
   r_t = \sigma(W_r\cdot[h_{t-1}, x_t])
   $$
3. 候选隐藏状态
   $$
   \tilde{h}_t = \tanh(W\cdot[r_t \circ h_{t-1}, x_t])
   $$
4. 隐藏状态
   $$
   h = (1 - z_t) \circ h_{t-1} + z_t \circ \tilde{h}_t
   $$

GRU的反向传播过程请读者自行推导。


